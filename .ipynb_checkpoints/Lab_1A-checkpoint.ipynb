{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/tirtharajdash/BITS-G513_MetaLearning/blob/main/Lab_1A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDzjUwlKqNwi"
   },
   "source": [
    "This notebook has been adapted from NYU's Deep Learning Course taught by Yann LeCun & Alfredo Canziani. \\[ [Website](https://atcold.github.io/pytorch-Deep-Learning/) ]\n",
    "\n",
    "# Outline\n",
    "1. Introduction about Pytorch. How does the Computation happen? How can gradients be computed? **[Lab 1A](https://github.com/tirtharajdash/BITS-G513_MetaLearning/blob/main/Lab_1A.ipynb)**\n",
    "2. Simple Regression and Classification **[Lab 1B](https://github.com/tirtharajdash/BITS-G513_MetaLearning/blob/main/Lab_1B.ipynb)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7x12gCyeXBf"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "A large part of the course will require understanding of how gradient based computation works. In particular, how models can be learnt through gradients. \n",
    "\n",
    "Apart from the initial few lectures on Deep Learning, computing gradients manually for learning will become more and more hard especially with newer modules such as Convolution, etc., being introduced.\n",
    "\n",
    "Given the scenario, we sought the use of Differentiable Computing Frameworks (PyTorch). These frameworks utilize computation graphs which are essentially directed acyclic graphs representing operations and variables. \n",
    "\n",
    "Lets suppose we were to do the following computation as part of our learning model:\n",
    "\n",
    "$$ p = x + y $$\n",
    "$$ g = p \\times z $$\n",
    "\n",
    "A simplistic representation of the computation graph would look like:\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://www.tutorialspoint.com/python_deep_learning/images/computational_graph_equation2.jpg\">\n",
    "</p>\n",
    "\n",
    "Here, the edges represent Tensors and nodes represent Operations.\n",
    "\n",
    "Lets code this up in PyTorch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wqe78umOr0SS",
    "outputId": "bd1cc318-65b7-4ea0-d9f8-9a866e24bd68",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchviz in /home/anany/anaconda3/lib/python3.7/site-packages (0.0.1)\r\n",
      "Requirement already satisfied: torch in /home/anany/anaconda3/lib/python3.7/site-packages (from torchviz) (1.7.1)\r\n",
      "Requirement already satisfied: graphviz in /home/anany/anaconda3/lib/python3.7/site-packages (from torchviz) (0.16)\r\n",
      "Requirement already satisfied: numpy in /home/anany/anaconda3/lib/python3.7/site-packages (from torch->torchviz) (1.18.1)\r\n",
      "Requirement already satisfied: typing-extensions in /home/anany/anaconda3/lib/python3.7/site-packages (from torch->torchviz) (3.7.4.3)\r\n"
     ]
    }
   ],
   "source": [
    "# Install a visualization software\n",
    "!pip install torchviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ji-krbdywTTO"
   },
   "source": [
    "## Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4HGKsQbcn8QM"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([2.], requires_grad=True)\n",
    "y = torch.tensor([3.], requires_grad=True)\n",
    "z = torch.tensor([4.], requires_grad=True)\n",
    "\n",
    "p = x + y\n",
    "g = p * z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07rfaT55iCZT"
   },
   "source": [
    "We can check the values of p and g just to be sure. They should be 5 and 20 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nN4xlkO3pvkF",
    "outputId": "c3c45201-e749-4651-b78c-e69a17708054"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p: tensor([5.], grad_fn=<AddBackward0>)\n",
      "g: tensor([20.], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"p: {p}\")\n",
    "print(f\"g: {g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_cqz2vwlSNi"
   },
   "source": [
    "Lets visualize the computation graph once to be sure. We will use `torchviz` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "id": "6Ldjt5oQlZei",
    "outputId": "ad8ae5e9-bd29-46b9-ceee-04acae67538a"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ceb4188c7921>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchviz\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_dot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmake_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchviz'"
     ]
    }
   ],
   "source": [
    "from torchviz import make_dot\n",
    "make_dot(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-Z65liViRqu"
   },
   "source": [
    "## Gradients\n",
    "\n",
    "In gradient based learning, what we've done till now is considered as a forward pass. The learning part is usually during the backward pass. So what is the backward pass?\n",
    "\n",
    "The Backward pass is essentially where we compute gradients and update our parameters of the model based on those gradients, so that the output is a step closer to where its supposed to be. In this notebook, we will be concerned only with inspecting the value of the gradients rather than updating them.\n",
    "\n",
    "To compute the gradients, we can call the `backward` method on a tensor. This `backward` call will compute gradients on all leaf variables (x, y, z) in the computation graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "eDijBPUciBpa"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c78eb0fdf568>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/meta/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/meta/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "g.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpbvoMzElrp7"
   },
   "source": [
    "The gradients can be computed simply in our case using backpropagation (chain rule):\n",
    "\n",
    "$$ \\frac{dg}{dp} = z = 4 $$\n",
    "\n",
    "$$ \\frac{dg}{dz} = p = 5 $$\n",
    "\n",
    "$$ \\frac{dg}{dx} = \\frac{dg}{dp} \\times \\frac{dp}{dx} = z \\times 1 = 4 $$\n",
    "\n",
    "$$ \\frac{dg}{dy} = \\frac{dg}{dp} \\times \\frac{dp}{dy} = z \\times 1 = 4 $$\n",
    "\n",
    "We can verify this by checking the gradients of all these variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nN-e4S0yh90m",
    "outputId": "025e1125-6742-43ec-b060-0ccc1f5ebfc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of dg wrt dz: tensor([5.])\n",
      "Gradient of dg wrt dx: tensor([4.])\n",
      "Gradient of dg wrt dy: tensor([4.])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Gradient of dg wrt dz: {z.grad}\")\n",
    "print(f\"Gradient of dg wrt dx: {x.grad}\")\n",
    "print(f\"Gradient of dg wrt dy: {y.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqAUSIsBtjRy"
   },
   "source": [
    "Gradients for non-leaf variables (e.g. p not x, y, z) usually require computation manually as they are not updated. Backpropagation relies on updating only leaf variables usually and if we update something in the middle that could break one of the equations.\n",
    "\n",
    "Another important point to note about PyTorch is that the graphs are dynamic. The Computation Graph is constructed when operations and variables are defined and its destructed when `backward` is called. So, calling backward again should give you an error.\n",
    "\n",
    "But to show how $$ \\frac{dg}{dp} $$ can be computed, lets do the computation again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "x6NiSDXhtipE"
   },
   "outputs": [],
   "source": [
    "x = torch.tensor([2.], requires_grad=True)\n",
    "y = torch.tensor([3.], requires_grad=True)\n",
    "z = torch.tensor([4.], requires_grad=True)\n",
    "\n",
    "p = x + y\n",
    "g = p * z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dCrtGiaMvW2U",
    "outputId": "f7b85fed-54e5-403d-8a4c-60129daaeea6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([4.]),)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To compute gradients for non-leaf variables such as p.\n",
    "\n",
    "torch.autograd.grad(g, p)\n",
    "\n",
    "# This should return 4 (from the equations above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bp8kGCzOw7LS"
   },
   "source": [
    "Computing Gradients of non-leaf variables can be useful for non-standard models (especially during the Meta-Learning part of the course) and may be used a lot in inspection of gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w82OTSemxOuN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Lab_1A.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "meta",
   "language": "python",
   "name": "meta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
