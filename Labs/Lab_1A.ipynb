{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab_1A.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tirtharajdash/BITS-G513_MetaLearning/blob/main/Lab_1A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDzjUwlKqNwi"
      },
      "source": [
        "This notebook has been adapted from NYU's Deep Learning Course taught by Yann LeCun & Alfredo Canziani. \\[ [Website](https://atcold.github.io/pytorch-Deep-Learning/) ]\n",
        "\n",
        "# Outline\n",
        "1. Introduction about Pytorch. How does the Computation happen? How can gradients be computed? **[Lab 1A](https://github.com/tirtharajdash/BITS-G513_MetaLearning/blob/main/Lab_1A.ipynb)**\n",
        "2. Simple Regression and Classification **[Lab 1B](https://github.com/tirtharajdash/BITS-G513_MetaLearning/blob/main/Lab_1B.ipynb)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7x12gCyeXBf"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "A large part of the course will require understanding of how gradient based computation works. In particular, how models can be learnt through gradients. \n",
        "\n",
        "Apart from the initial few lectures on Deep Learning, computing gradients manually for learning will become more and more hard especially with newer modules such as Convolution, etc., being introduced.\n",
        "\n",
        "Given the scenario, we sought the use of Differentiable Computing Frameworks (PyTorch). These frameworks utilize computation graphs which are essentially directed acyclic graphs representing operations and variables. \n",
        "\n",
        "Lets suppose we were to do the following computation as part of our learning model:\n",
        "\n",
        "$$ p = x + y $$\n",
        "$$ g = p \\times z $$\n",
        "\n",
        "A simplistic representation of the computation graph would look like:\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://www.tutorialspoint.com/python_deep_learning/images/computational_graph_equation2.jpg\">\n",
        "</p>\n",
        "\n",
        "Here, the edges represent Tensors and nodes represent Operations.\n",
        "\n",
        "Lets code this up in PyTorch!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqe78umOr0SS",
        "outputId": "bd1cc318-65b7-4ea0-d9f8-9a866e24bd68"
      },
      "source": [
        "# Install a visualization software\n",
        "!pip install torchviz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchviz in /usr/local/lib/python3.6/dist-packages (0.0.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchviz) (1.7.0+cu101)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->torchviz) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ji-krbdywTTO"
      },
      "source": [
        "## Forward Pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HGKsQbcn8QM"
      },
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor([2.], requires_grad=True)\n",
        "y = torch.tensor([3.], requires_grad=True)\n",
        "z = torch.tensor([4.], requires_grad=True)\n",
        "\n",
        "p = x + y\n",
        "g = p * z"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07rfaT55iCZT"
      },
      "source": [
        "We can check the values of p and g just to be sure. They should be 5 and 20 respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nN4xlkO3pvkF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3c45201-e749-4651-b78c-e69a17708054"
      },
      "source": [
        "print(f\"p: {p}\")\n",
        "print(f\"g: {g}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "p: tensor([5.], grad_fn=<AddBackward0>)\n",
            "g: tensor([20.], grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_cqz2vwlSNi"
      },
      "source": [
        "Lets visualize the computation graph once to be sure. We will use `torchviz` for this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "6Ldjt5oQlZei",
        "outputId": "ad8ae5e9-bd29-46b9-ceee-04acae67538a"
      },
      "source": [
        "from torchviz import make_dot\n",
        "make_dot(g)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<graphviz.dot.Digraph at 0x7f2876f22cf8>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"189pt\" height=\"171pt\"\n viewBox=\"0.00 0.00 189.00 171.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 167)\">\n<title>%3</title>\n<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-167 185,-167 185,4 -4,4\"/>\n<!-- 139811771001040 -->\n<g id=\"node1\" class=\"node\">\n<title>139811771001040</title>\n<polygon fill=\"#caff70\" stroke=\"#000000\" points=\"153.5,-21 62.5,-21 62.5,0 153.5,0 153.5,-21\"/>\n<text text-anchor=\"middle\" x=\"108\" y=\"-7.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">MulBackward0</text>\n</g>\n<!-- 139811771001992 -->\n<g id=\"node2\" class=\"node\">\n<title>139811771001992</title>\n<polygon fill=\"#d3d3d3\" stroke=\"#000000\" points=\"109,-85 17,-85 17,-64 109,-64 109,-85\"/>\n<text text-anchor=\"middle\" x=\"63\" y=\"-71.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\">AddBackward0</text>\n</g>\n<!-- 139811771001992&#45;&gt;139811771001040 -->\n<g id=\"edge1\" class=\"edge\">\n<title>139811771001992&#45;&gt;139811771001040</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M70.4308,-63.9317C76.9705,-54.6309 86.6534,-40.8597 94.5509,-29.6276\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"97.519,-31.4914 100.4077,-21.2979 91.7928,-27.4651 97.519,-31.4914\"/>\n</g>\n<!-- 139811771001936 -->\n<g id=\"node3\" class=\"node\">\n<title>139811771001936</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"54,-163 0,-163 0,-128 54,-128 54,-163\"/>\n<text text-anchor=\"middle\" x=\"27\" y=\"-135.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1)</text>\n</g>\n<!-- 139811771001936&#45;&gt;139811771001992 -->\n<g id=\"edge2\" class=\"edge\">\n<title>139811771001936&#45;&gt;139811771001992</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M35.8989,-127.9494C41.1277,-117.6371 47.747,-104.5824 53.1348,-93.9563\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"56.2607,-95.5307 57.6614,-85.0288 50.0174,-92.365 56.2607,-95.5307\"/>\n</g>\n<!-- 139811771002048 -->\n<g id=\"node4\" class=\"node\">\n<title>139811771002048</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"126,-163 72,-163 72,-128 126,-128 126,-163\"/>\n<text text-anchor=\"middle\" x=\"99\" y=\"-135.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1)</text>\n</g>\n<!-- 139811771002048&#45;&gt;139811771001992 -->\n<g id=\"edge3\" class=\"edge\">\n<title>139811771002048&#45;&gt;139811771001992</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M90.1011,-127.9494C84.8723,-117.6371 78.253,-104.5824 72.8652,-93.9563\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"75.9826,-92.365 68.3386,-85.0288 69.7393,-95.5307 75.9826,-92.365\"/>\n</g>\n<!-- 139811771001320 -->\n<g id=\"node5\" class=\"node\">\n<title>139811771001320</title>\n<polygon fill=\"#add8e6\" stroke=\"#000000\" points=\"181,-92 127,-92 127,-57 181,-57 181,-92\"/>\n<text text-anchor=\"middle\" x=\"154\" y=\"-64.4\" font-family=\"Times,serif\" font-size=\"12.00\" fill=\"#000000\"> (1)</text>\n</g>\n<!-- 139811771001320&#45;&gt;139811771001040 -->\n<g id=\"edge4\" class=\"edge\">\n<title>139811771001320&#45;&gt;139811771001040</title>\n<path fill=\"none\" stroke=\"#000000\" d=\"M141.1864,-56.6724C135.1095,-48.2176 127.8436,-38.1085 121.6618,-29.5078\"/>\n<polygon fill=\"#000000\" stroke=\"#000000\" points=\"124.3859,-27.3009 115.7075,-21.2234 118.7018,-31.3863 124.3859,-27.3009\"/>\n</g>\n</g>\n</svg>\n"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-Z65liViRqu"
      },
      "source": [
        "## Gradients\n",
        "\n",
        "In gradient based learning, what we've done till now is considered as a forward pass. The learning part is usually during the backward pass. So what is the backward pass?\n",
        "\n",
        "The Backward pass is essentially where we compute gradients and update our parameters of the model based on those gradients, so that the output is a step closer to where its supposed to be. In this notebook, we will be concerned only with inspecting the value of the gradients rather than updating them.\n",
        "\n",
        "To compute the gradients, we can call the `backward` method on a tensor. This `backward` call will compute gradients on all leaf variables (x, y, z) in the computation graph. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDijBPUciBpa"
      },
      "source": [
        "g.backward()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpbvoMzElrp7"
      },
      "source": [
        "The gradients can be computed simply in our case using backpropagation (chain rule):\n",
        "\n",
        "$$ \\frac{dg}{dp} = z = 4 $$\n",
        "\n",
        "$$ \\frac{dg}{dz} = p = 5 $$\n",
        "\n",
        "$$ \\frac{dg}{dx} = \\frac{dg}{dp} \\times \\frac{dp}{dx} = z \\times 1 = 4 $$\n",
        "\n",
        "$$ \\frac{dg}{dy} = \\frac{dg}{dp} \\times \\frac{dp}{dy} = z \\times 1 = 4 $$\n",
        "\n",
        "We can verify this by checking the gradients of all these variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nN-e4S0yh90m",
        "outputId": "025e1125-6742-43ec-b060-0ccc1f5ebfc4"
      },
      "source": [
        "print(f\"Gradient of dg wrt dz: {z.grad}\")\n",
        "print(f\"Gradient of dg wrt dx: {x.grad}\")\n",
        "print(f\"Gradient of dg wrt dy: {y.grad}\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gradient of dg wrt dz: tensor([5.])\n",
            "Gradient of dg wrt dx: tensor([4.])\n",
            "Gradient of dg wrt dy: tensor([4.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqAUSIsBtjRy"
      },
      "source": [
        "Gradients for non-leaf variables (e.g. p not x, y, z) usually require computation manually as they are not updated. Backpropagation relies on updating only leaf variables usually and if we update something in the middle that could break one of the equations.\n",
        "\n",
        "Another important point to note about PyTorch is that the graphs are dynamic. The Computation Graph is constructed when operations and variables are defined and its destructed when `backward` is called. So, calling backward again should give you an error.\n",
        "\n",
        "But to show how $$ \\frac{dg}{dp} $$ can be computed, lets do the computation again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6NiSDXhtipE"
      },
      "source": [
        "x = torch.tensor([2.], requires_grad=True)\n",
        "y = torch.tensor([3.], requires_grad=True)\n",
        "z = torch.tensor([4.], requires_grad=True)\n",
        "\n",
        "p = x + y\n",
        "g = p * z"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCrtGiaMvW2U",
        "outputId": "f7b85fed-54e5-403d-8a4c-60129daaeea6"
      },
      "source": [
        "# To compute gradients for non-leaf variables such as p.\n",
        "\n",
        "torch.autograd.grad(g, p)\n",
        "\n",
        "# This should return 4 (from the equations above)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([4.]),)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bp8kGCzOw7LS"
      },
      "source": [
        "Computing Gradients of non-leaf variables can be useful for non-standard models (especially during the Meta-Learning part of the course) and may be used a lot in inspection of gradients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w82OTSemxOuN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}