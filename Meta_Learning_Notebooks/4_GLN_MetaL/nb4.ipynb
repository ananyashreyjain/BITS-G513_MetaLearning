{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONTINUAL LEARNING using Gated Linear Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "#%cd drive/MyDrive/'Colab Notebooks'\n",
    "#%cd meta-learning-course-notebooks/1_MAML/\n",
    "#!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install import_ipynb --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install learn2learn --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import utils\n",
    "import models\n",
    "utils.hide_toggle('Imports 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import torch\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pyplot as plt\n",
    "from l2lutils import KShotLoader\n",
    "from IPython import display\n",
    "utils.hide_toggle('Imports 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate data - euclidean\n",
    "meta_train_ds, meta_test_ds, full_loader = utils.euclideanDataset(n_samples=10000,n_features=20,n_classes=10,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an MLP network. Note that input dimension has to be data dimension. For classification\n",
    "# final dimension has to be number of classes; for regression one.\n",
    "#torch.manual_seed(10)\n",
    "net = models.MLP(dims=[20,32,32,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the network; note that network is trained in place so repeated calls further train it.\n",
    "net,losses,accs=models.Train(net,full_loader,lr=1e-2,epochs=10,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training accuracy.\n",
    "models.accuracy(net,meta_train_ds.samples,meta_train_ds.labels,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test accuracy.\n",
    "models.accuracy(net,meta_test_ds.samples,meta_test_ds.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta-Learning: Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a k-shot n-way loader using the meta-training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_train_kloader=KShotLoader(meta_train_ds,shots=5,ways=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample a task - each task has a k-shot n-way training set and a similar test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train,d_test=meta_train_kloader.get_task()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try directly learning using the task training set albeit its small size: create a dataset and loader and train it with the earlier network and Train function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taskds = utils.MyDS(d_train[0],d_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train_loader = torch.utils.data.DataLoader(dataset=taskds,batch_size=1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net,loss=models.Train(net,d_train_loader,lr=1e-1,epochs=10,verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does it do on the test set of the sampled task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.accuracy(net,d_test[0],d_test[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gated Linear Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygln import GLN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GLN Interface\n",
    "\n",
    "### Constructor\n",
    "\n",
    "```python\n",
    "GLN(backend: str,\n",
    "    layer_sizes: Sequence[int],\n",
    "    input_size: int,\n",
    "    context_map_size: int = 4,\n",
    "    num_classes: int = 2,\n",
    "    base_predictor: Optional[Callable] = None,\n",
    "    learning_rate: float = 1e-4,\n",
    "    pred_clipping: float = 1e-3,\n",
    "    weight_clipping: float = 5.0,\n",
    "    bias: bool = True,\n",
    "    context_bias: bool = True)\n",
    "```\n",
    "\n",
    "Gated Linear Network constructor.\n",
    "\n",
    "**Args:**\n",
    "\n",
    "- **backend** (*\"jax\", \"numpy\", \"pytorch\", \"tf\"*): Which backend implementation to use.\n",
    "- **layer\\_sizes** (*list[int >= 1]*): List of layer output sizes.\n",
    "- **input\\_size** (*int >= 1*): Input vector size.\n",
    "- **num\\_classes** (*int >= 2*): For values >2, turns GLN into a multi-class classifier by\n",
    "    internally creating a one-vs-all binary GLN classifier per class and return the argmax as\n",
    "    output.\n",
    "- **context\\_map\\_size** (*int >= 1*): Context dimension, i.e. number of context halfspaces.\n",
    "- **bias** (*bool*): Whether to add a bias prediction in each layer.\n",
    "- **context\\_bias** (*bool*): Whether to use a random non-zero bias for context halfspace gating.\n",
    "- **base\\_predictor** (*np.array[N] -> np.array[K]*): If given, maps the N-dim input vector to a\n",
    "    corresponding K-dim vector of base predictions (could be a constant prior), instead of\n",
    "    simply using the clipped input vector itself.\n",
    "- **learning\\_rate** (*float > 0.0*): Update learning rate.\n",
    "- **pred\\_clipping** (*0.0 < float < 0.5*): Clip predictions into [p, 1 - p] at each layer.\n",
    "- **weight\\_clipping** (*float > 0.0*): Clip weights into [-w, w] after each update.\n",
    "\n",
    "---\n",
    "\n",
    "### Predict\n",
    "\n",
    "```python\n",
    "GLN.predict(input: np.ndarray,\n",
    "            target: np.ndarray = None,\n",
    "            return_probs: bool = False) -> np.ndarray\n",
    "```\n",
    "\n",
    "Predict the class for the given inputs, and optionally update the weights.\n",
    "\n",
    "> **PyTorch** implementation takes `torch.Tensor`s (on the same device as the model) as parameters.\n",
    "\n",
    "**Args:**\n",
    "\n",
    "- **input** (*np.array[B, N]*): Batch of B N-dim float input vectors.\n",
    "- **target** (*np.array[B]*): Optional batch of B bool/int target class labels which, if given,\n",
    "    triggers an online update if given.\n",
    "- **return\\_probs** (*bool*): Whether to return the classification probability (for each\n",
    "    one-vs-all classifier if num_classes given) instead of the class.\n",
    "\n",
    "**Returns:**\n",
    "\n",
    "- Predicted class per input instance, or classification probabilities if return_probs set.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling a training task: Note that each of d_train and d_test is a tuple comprising of a training set, and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_train_kloader=KShotLoader(meta_train_ds,shots=100,ways=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_test_kloader=KShotLoader(meta_test_ds,shots=100,ways=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train,d_test=meta_train_kloader.get_task()\n",
    "rp = torch.randperm(d_train[1].shape[0])\n",
    "d_train0=d_train[0][rp]\n",
    "d_train1=d_train[1][rp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=(lambda x: (x - x.min(axis=1, keepdims=True)[0]) /(x.max(axis=1, keepdims=True)[0] - x.min(axis=1, keepdims=True)[0])+0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=(lambda x: x/2*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gln = GLN(backend='pytorch', layer_sizes=[4, 4, 1], input_size=20,base_predictor=f)\n",
    "lossfn = torch.nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    preds = gln.predict(d_train0[i:i+50],d_train1[i:i+50],return_probs=True)\n",
    "    #print(preds)\n",
    "    #print(d_train[1][i:1+10].unsqueeze(0))\n",
    "    pt = torch.tensor(preds).unsqueeze(-1)\n",
    "    ph = torch.cat((torch.log(1-pt),torch.log(pt)),dim=1)\n",
    "    loss = lossfn(ph,d_train1[i:i+50])\n",
    "    #print(ph[0:10])\n",
    "    #print(d_train1[i:i+10])\n",
    "    print(loss)\n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train,d_test=meta_test_kloader.get_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = gln.predict(d_train[0],d_train[1],return_probs=True)\n",
    "pt = torch.tensor(preds).unsqueeze(-1)\n",
    "ph = torch.cat((pt,1-pt),dim=1)\n",
    "lossfn(ph,d_train[1])\n",
    "d_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = d_train[0][0].unsqueeze(0)\n",
    "#(t.max(axis=1, keepdims=False) - t.min(axis=1, keepdims=False))\n",
    "#torch.min(t,axis=1,keepdims=False)[0]\n",
    "f(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together: \n",
    "# Continually Training a Gated Linear Network\n",
    "Now let's put all of the above in a loop - continually training a Gated Linear Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskStore():\n",
    "    def __init__(self):\n",
    "        self.XL = []\n",
    "        self.yL = []\n",
    "        self.i = 0\n",
    "        self.n = 0\n",
    "    def save_task(self,X,y):\n",
    "        self.XL += [X]\n",
    "        self.yL += [y]\n",
    "        self.n += 1\n",
    "    def get_task(self):\n",
    "        if self.i < self.n:\n",
    "            X = self.XL[self.i]\n",
    "            y = self.yL[self.i]\n",
    "            self.i += 1\n",
    "            return X,y,False\n",
    "        else:\n",
    "            return self.XL[0],self.yL[0],True\n",
    "    def reset(self):\n",
    "        self.i = 0\n",
    "        self.n = 0\n",
    "    def load(self,loader,n_tasks=5):\n",
    "        for t in range(n_tasks):\n",
    "            d_train,d_test=loader.get_task()\n",
    "            rp = torch.randperm(d_train[1].shape[0])\n",
    "            d_train0=d_train[0][rp]\n",
    "            d_train1=d_train[1][rp]\n",
    "            rp1 = torch.randperm(d_test[1].shape[0])\n",
    "            d_test0=d_test[0][rp1]\n",
    "            d_test1=d_test[1][rp1]\n",
    "            X = torch.cat((d_train0,d_test0),axis=0)\n",
    "            y = torch.cat((d_train1,d_test1),axis=0)\n",
    "            self.save_task(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = (lambda x: (x - x.min(axis=1, keepdims=True)[0]) / (x.max(axis=1, keepdims=True)[0] - x.min(axis=1, keepdims=True)[0])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gln_acc(Net,X_test,y_test,verbose=False):\n",
    "    m = X_test.shape[0]\n",
    "    preds = Net.predict(X_test,return_probs=True)\n",
    "    pt = torch.tensor(preds).unsqueeze(-1)\n",
    "    ph = torch.cat((torch.log(1-pt),torch.log(pt)),dim=1)\n",
    "    _,predicted = torch.max(ph,axis=1)\n",
    "    correct = (predicted == y_test).float().sum().item()\n",
    "    accuracy = correct/m\n",
    "    if verbose: print(accuracy)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import learn2learn as l2l\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "shots,ways = 100,2\n",
    "n_features = 20\n",
    "net = GLN(learning_rate=1e-2,backend='pytorch', layer_sizes=[8, 8, 8, 1], input_size=20,base_predictor=f,context_map_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Meta-testing task loader for later.\n",
    "meta_test_kloader=KShotLoader(meta_test_ds,shots=shots,ways=ways)\n",
    "meta_train_kloader=KShotLoader(meta_train_ds,shots=shots,ways=ways,num_tasks=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainStore = TaskStore()\n",
    "testStore = TaskStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainStore.load(meta_train_kloader,n_tasks=25)\n",
    "testStore.load(meta_test_kloader,n_tasks=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runGLN(net,task_store,learn=True,task_count=5,batch_size=32,epochs=50):\n",
    "    lossfn = torch.nn.NLLLoss()\n",
    "    bs = batch_size\n",
    "    lossL=[]\n",
    "    accs_trainL=[]\n",
    "    accs_testL=[]\n",
    "    accsL=[]\n",
    "    n_batch = 0\n",
    "    start = shots\n",
    "    for task in range(task_count):\n",
    "        task_loss = 0.0\n",
    "        # load task and train/predict on it\n",
    "        X,y,_ = task_store.get_task()\n",
    "        i = 0\n",
    "        n = int(X.shape[0]/2)\n",
    "        for e in range(epochs):\n",
    "            while (i<n-bs):\n",
    "                if learn==True: preds = net.predict(X[i:i+bs],y[i:i+bs],return_probs=True)\n",
    "                else: preds = net.predict(X[i:i+bs],return_probs=True)\n",
    "                pt = torch.tensor(preds).unsqueeze(-1)\n",
    "                ph = torch.cat((torch.log(1-pt),torch.log(pt)),dim=1)\n",
    "                loss = lossfn(ph,y[i:i+bs])\n",
    "                lossL+=[loss]\n",
    "                n_batch+=1\n",
    "                i+=bs\n",
    "        # Training accuracy\n",
    "        acc_train = gln_acc(net,X[0:n],y[0:n])\n",
    "        # Test accuracy\n",
    "        acc_test = gln_acc(net,X[n:2*n],y[n:2*n])\n",
    "        acc = (acc_train+acc_test)/2\n",
    "        accsL += [acc]\n",
    "        accs_trainL += [acc_train]\n",
    "        accs_testL += [acc_test]\n",
    "        print(\"Task:\",task,sum(lossL)/n_batch,acc_train,acc_test)\n",
    "    return lossL,accsL,accs_trainL,accs_testL\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainStore.i=0\n",
    "testStore.i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossL1,accsA1,accsTr1,accsTe1=runGLN(net,trainStore,epochs=100,task_count=25)\n",
    "#gln_acc(net,X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainStore.i=0\n",
    "lossL2,accsA2,accsTr2,accsTe2=runGLN(net,trainStore,learn=False,epochs=100,task_count=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,(ax1,ax2,ax3,ax4,ax5) = plt.subplots(1,5,sharey=True,figsize=(14,4))\n",
    "ax1.plot(accsA1,color='blue') # - train on 25 tasks\n",
    "ax2.plot(accsA2,color='red')  # - test on same 25 tasks\n",
    "ax3.plot(accsTe1,color='green') # - test on test data of same 25 tasks\n",
    "ax4.plot(lossL1,color='black') # - losses during first training\n",
    "ax5.plot(lossL2,color='orange') # - losses post training on all tasks without more training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainStore.i=0\n",
    "lossL3,accsA3,accsTr3,accsTe3=runGLN(net,trainStore,epochs=100,task_count=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,(ax1,ax2,ax3,ax4,ax5) = plt.subplots(1,5,sharey=True,figsize=(14,4))\n",
    "ax1.plot(lossL1,color='blue') # - train on 25 tasks\n",
    "ax2.plot(lossL3,color='red')  # - train again on same 25 tasks\n",
    "ax3.plot(accsA3,color='green') # - accuracies on after retraining\n",
    "ax4.plot(accsTr3,color='black') # - training accuracies\n",
    "ax5.plot(accsTe3,color='orange') # - test accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainStore.i=0\n",
    "testStore.i=0\n",
    "lossL4,accsA4,accsTr4,accsTe4=runGLN(net,testStore,epochs=100,task_count=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,(ax1,ax2,ax3,ax4,ax5) = plt.subplots(1,5,sharey=True,figsize=(14,4))\n",
    "ax1.plot(lossL1,color='blue') # - train on 25 tasks\n",
    "ax2.plot(lossL4,color='red')  # - train on new 25 tasks\n",
    "ax3.plot(accsA1,color='green') # - accuraciess on training data of first 25 tasks\n",
    "ax4.plot(accsA4,color='black') # - accuracies new tasks 25 tasks\n",
    "ax5.plot(accsTe4,color='orange') # - test accuracies on new 25 tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW4: Try with MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on the the degree of forgetting using MLP vs that using GLN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
