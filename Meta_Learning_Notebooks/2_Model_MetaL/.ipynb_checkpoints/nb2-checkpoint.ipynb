{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> TO BE DONE </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL-BASED META-LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "#%cd drive/MyDrive/'Colab Notebooks'\n",
    "#%cd meta-learning-course-notebooks/1_MAML/\n",
    "#!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install import_ipynb --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install learn2learn --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from utils.ipynb\n",
      "importing Jupyter notebook from models.ipynb\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_1132428324359060697() {\n",
       "                $('div.cell.code_cell.rendered.selected').find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            \n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_1132428324359060697()\"><b>Imports 1</b> (show/hide)</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import import_ipynb\n",
    "import utils\n",
    "import models\n",
    "utils.hide_toggle('Imports 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from l2lutils.ipynb\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <script>\n",
       "            function code_toggle_18166247632528183713() {\n",
       "                $('div.cell.code_cell.rendered.selected').find('div.input').toggle();\n",
       "            }\n",
       "\n",
       "            \n",
       "        </script>\n",
       "\n",
       "        <a href=\"javascript:code_toggle_18166247632528183713()\"><b>Imports 2</b> (show/hide)</a>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython import display\n",
    "import torch\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pyplot as plt\n",
    "from l2lutils import KShotLoader\n",
    "from IPython import display\n",
    "utils.hide_toggle('Imports 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation/Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate data - euclidean\n",
    "meta_train_ds, meta_test_ds, full_loader = utils.euclideanDataset(n_samples=10000,n_features=20,n_classes=10,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate data - sinusoidal mix\n",
    "meta_train_ds, meta_test_ds, full_loader = utils.sinDataset(n_samples=10000,length=20,n_classes=10,batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_train_ds, meta_test_ds, full_loader = utils.mnist_data(frac=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an MLP network. Note that input dimension has to be data dimension. For classification\n",
    "# final dimension has to be number of classes; for regression one.\n",
    "#torch.manual_seed(10)\n",
    "net = models.MLP(dims=[784,500,300,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   49 Loss: 1.04247e+00 Accuracy: 0.99200\n"
     ]
    }
   ],
   "source": [
    "# Train the network; note that network is trained in place so repeated calls further train it.\n",
    "net,loss=models.Train(net,full_loader,lr=1e-2,epochs=50,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.0 75\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training accuracy.\n",
    "models.accuracy(net,meta_train_ds.samples,meta_train_ds.labels,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.0 25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test accuracy.\n",
    "models.accuracy(net,meta_test_ds.samples,meta_test_ds.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta-Learning: Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a k-shot n-way loader using the meta-training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_train_kloader=KShotLoader(meta_train_ds,shots=5,ways=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample a task - each task has a k-shot n-way training set and a similar test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train,d_test=meta_train_kloader.get_task()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try directly learning using the task training set albeit its small size: create a dataset and loader and train it with the earlier network and Train function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "taskds = utils.MyDS(d_train[0],d_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train_loader = torch.utils.data.DataLoader(dataset=taskds,batch_size=1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   9 Loss: 8.05541e-01 Accuracy: 0.60000\n"
     ]
    }
   ],
   "source": [
    "net,loss=models.Train(net,d_train_loader,lr=1e-1,epochs=10,verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does it do on the test set of the sampled task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.accuracy(net,d_test[0],d_test[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN-based  Meta-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimisers from torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an RNN to process a DATASET; default is 1-D where input can be (batch,seq_vals) else input will need to be given\n",
    "# with an extra dimension and input will be (batch,seq,features)\n",
    "net = models.RNN(n_classes=10,dim=10,n_layers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(),lr=1e-3)\n",
    "lossfn = torch.nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a task dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train,d_test=meta_train_kloader.get_task()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = learner(d_train[0])\n",
    "train_loss = lossfn(train_preds,d_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0471, -0.0599,  0.0888,  ...,  0.0040,  0.1487, -0.2101],\n",
       "        [-0.1053, -0.0518,  0.0762,  ...,  0.1725,  0.2212, -0.0021],\n",
       "        [ 0.1549,  0.1261,  0.0488,  ..., -0.0286,  0.0378,  0.2029],\n",
       "        ...,\n",
       "        [ 0.0047,  0.1464, -0.1188,  ..., -0.1645,  0.0632, -0.1158],\n",
       "        [ 0.1076, -0.2054, -0.0637,  ...,  0.0623, -0.1807,  0.0112],\n",
       "        [-0.1606,  0.1825,  0.0678,  ..., -0.1381, -0.1322, -0.2206]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.layers[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1157,  0.1630,  0.0410,  ...,  0.1286,  0.1797, -0.0187],\n",
       "        [ 0.0166, -0.1468, -0.0344,  ...,  0.0152, -0.0072, -0.1046],\n",
       "        [ 0.1589, -0.1557,  0.1609,  ...,  0.0345,  0.0556,  0.1322],\n",
       "        ...,\n",
       "        [ 0.2711,  0.0826, -0.0049,  ...,  0.0591, -0.1569, -0.1229],\n",
       "        [-0.0897,  0.0349, -0.1068,  ...,  0.0615,  0.0739,  0.1915],\n",
       "        [ 0.1748, -0.0248, -0.0785,  ...,  0.0104, -0.1549, -0.1313]],\n",
       "       grad_fn=<CloneBackward>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.layers[0].weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that at this point both the learner and original net have the same parameters. Lets see what the gradients w.r.t the TRAINING loss are: (We use pytorch's autograd functions directly.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0214,  0.0020, -0.0031,  ...,  0.0282, -0.0057,  0.0132],\n",
       "        [ 0.0109,  0.0212, -0.0209,  ..., -0.0435,  0.0343, -0.0673],\n",
       "        [-0.0120, -0.0074, -0.0048,  ...,  0.0068,  0.0026, -0.0079],\n",
       "        ...,\n",
       "        [ 0.0296,  0.0115, -0.0008,  ..., -0.0234,  0.0052, -0.0268],\n",
       "        [-0.0031,  0.0098, -0.0038,  ..., -0.0130,  0.0084, -0.0066],\n",
       "        [ 0.0396,  0.0230, -0.0315,  ..., -0.0883,  0.0633, -0.1212]],\n",
       "       grad_fn=<TBackward>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_grad=grad(train_loss,learner.layers[0].weight,retain_graph=True,\n",
    "                                 create_graph=True,\n",
    "                                 allow_unused=True)\n",
    "train_grad[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we ADAPT the learner by taking one step on the CLONED parameters in direction of the gradient of the TRAINING loss above. This is the part that the l2l libarary does for us as per the MAML algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.adapt(train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check what has happended:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1155,  0.1630,  0.0411,  ...,  0.1283,  0.1798, -0.0188],\n",
       "        [ 0.0165, -0.1470, -0.0342,  ...,  0.0156, -0.0075, -0.1040],\n",
       "        [ 0.1590, -0.1556,  0.1610,  ...,  0.0345,  0.0556,  0.1323],\n",
       "        ...,\n",
       "        [ 0.2708,  0.0825, -0.0048,  ...,  0.0593, -0.1570, -0.1226],\n",
       "        [-0.0897,  0.0348, -0.1068,  ...,  0.0616,  0.0738,  0.1916],\n",
       "        [ 0.1744, -0.0251, -0.0782,  ...,  0.0113, -0.1555, -0.1301]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.layers[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.5863e+00, -1.0889e+02, -1.5623e+01,  ..., -4.4132e+00,\n",
       "          5.4812e+00, -1.4445e+01],\n",
       "        [-1.1130e+01,  4.4791e+00, -5.2846e+00,  ..., -3.6071e+00,\n",
       "          6.6775e+00, -1.5145e+00],\n",
       "        [ 3.4491e-01, -3.8301e+01,  2.3495e+01,  ..., -9.2308e+00,\n",
       "         -6.7455e+00, -8.9012e+00],\n",
       "        ...,\n",
       "        [-8.9923e+00,  5.5352e+00,  1.5008e+02,  ...,  9.5664e+00,\n",
       "          4.2451e+01, -2.5332e-01],\n",
       "        [-6.2786e+01, -2.4582e+01, -1.1441e+01,  ..., -4.9457e-02,\n",
       "         -3.0335e+01,  2.7421e+01],\n",
       "        [-8.4692e+00,  9.0063e+00, -4.6373e+00,  ...,  1.6917e+00,\n",
       "          3.6820e-01,  7.4680e-01]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(net.layers[0].weight - learner.layers[0].weight)/train_grad[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So one step in the diretion of the gradient (w.r.t train_loss) has been taken. Next we compute the loss of this ADAPTED learner w.r.t. the TEST data of the task, i.e., d_test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = learner(d_test[0])\n",
    "adapt_loss = lossfn(test_preds,d_test[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main MAML update to the original network net takes place now, by back-propagating through the (cumulative) adaptation loss (across possibly many tasks, here there was just one):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_count = 1\n",
    "optimizer.zero_grad()\n",
    "total_loss = adapt_loss/task_count\n",
    "total_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0471, -0.0599,  0.0888,  ...,  0.0040,  0.1487, -0.2101],\n",
       "        [-0.1053, -0.0518,  0.0762,  ...,  0.1725,  0.2212, -0.0021],\n",
       "        [ 0.1549,  0.1261,  0.0488,  ..., -0.0286,  0.0378,  0.2029],\n",
       "        ...,\n",
       "        [ 0.0047,  0.1464, -0.1188,  ..., -0.1645,  0.0632, -0.1158],\n",
       "        [ 0.1076, -0.2054, -0.0637,  ...,  0.0623, -0.1807,  0.0112],\n",
       "        [-0.1606,  0.1825,  0.0678,  ..., -0.1381, -0.1322, -0.2206]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.layers[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0471, -0.0599,  0.0888,  ...,  0.0040,  0.1487, -0.2101],\n",
       "        [-0.1053, -0.0518,  0.0762,  ...,  0.1725,  0.2212, -0.0021],\n",
       "        [ 0.1549,  0.1261,  0.0488,  ..., -0.0286,  0.0378,  0.2029],\n",
       "        ...,\n",
       "        [ 0.0047,  0.1464, -0.1188,  ..., -0.1645,  0.0632, -0.1158],\n",
       "        [ 0.1076, -0.2054, -0.0637,  ...,  0.0623, -0.1807,  0.0112],\n",
       "        [-0.1606,  0.1825,  0.0678,  ..., -0.1381, -0.1322, -0.2206]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.layers[0].weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the original parameters have been updated by a gradient step using on all the task adaptation losses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together: RNN-based Meta-learning\n",
    "Now let's put all of the above in a loop - the MAML algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import learn2learn as l2l\n",
    "import torch.optim as optim\n",
    "shots,ways = 5,2\n",
    "net = models.MLP(dims=[20,64,32,ways])\n",
    "#net = models.RNN(n_classes=3,dim=10,n_layers=2)\n",
    "maml = l2l.algorithms.MAML(net, lr=1e-2)\n",
    "optimizer = optim.Adam(maml.parameters(),lr=5e-3)\n",
    "lossfn = torch.nn.NLLLoss()\n",
    "meta_train_kloader=KShotLoader(meta_train_ds,shots=shots,ways=ways,num_tasks=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs, tasks per step and number of fast_adaptation steps \n",
    "n_epochs=10\n",
    "task_count=32\n",
    "fas = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In practice we use more than one gradient step for adpation, this is called 'fast adaptation'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   9 Loss: 4.84297e-01 Avg Acc: 0.77500\n"
     ]
    }
   ],
   "source": [
    "epoch=0\n",
    "while epoch<n_epochs:\n",
    "    adapt_loss = 0.0\n",
    "    test_acc = 0.0\n",
    "    # Sample and train on a task\n",
    "    for task in range(task_count):\n",
    "        d_train,d_test=meta_train_kloader.get_task()\n",
    "        learner = maml.clone()\n",
    "        for fas_step in range(fas):\n",
    "            train_preds = learner(d_train[0])\n",
    "            train_loss = lossfn(train_preds,d_train[1])\n",
    "            learner.adapt(train_loss)\n",
    "        test_preds = learner(d_test[0])\n",
    "        adapt_loss += lossfn(test_preds,d_test[1])\n",
    "        learner.eval()\n",
    "        test_acc += models.accuracy(learner,d_test[0],d_test[1],verbose=False)\n",
    "        learner.train()\n",
    "        # Done with a task\n",
    "    # Update main network\n",
    "    print('Epoch  % 2d Loss: %2.5e Avg Acc: %2.5f'%(epoch,adapt_loss/task_count,test_acc/task_count))\n",
    "    display.clear_output(wait=True)\n",
    "    optimizer.zero_grad()\n",
    "    total_loss = adapt_loss\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    epoch+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test the trained maml network and applying the adaption step to tasks sampled from the meta_test_ds dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Acc: 0.57500\n"
     ]
    }
   ],
   "source": [
    "shots\n",
    "meta_test_kloader=KShotLoader(meta_test_ds,shots=shots,ways=ways)\n",
    "test_acc = 0.0\n",
    "task_count = 20\n",
    "adapt_steps = 5\n",
    "maml.eval()\n",
    "# Sample and train on a task\n",
    "for task in range(task_count):\n",
    "    d_train,d_test=meta_test_kloader.get_task()\n",
    "    learner = maml.clone()\n",
    "    learner.eval()\n",
    "    for adapt_step in range(adapt_steps):\n",
    "        train_preds = learner(d_train[0])\n",
    "        train_loss = lossfn(train_preds,d_train[1])\n",
    "        learner.adapt(train_loss)\n",
    "    test_preds = learner(d_test[0])\n",
    "    test_acc += models.accuracy(learner,d_test[0],d_test[1],verbose=False)\n",
    "    # Done with a task\n",
    "learner.train()\n",
    "print('Avg Acc: %2.5f'%(test_acc/task_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
